# Enterprise RAG System â€” Project Summary

## What We Built

A production-grade **Retrieval Augmented Generation (RAG)** system with an agentic
pipeline, multi-backend support, and a Streamlit UI. Built incrementally over multiple
sessions â€” every decision was deliberate and understood.

---

## Architecture

```
Documents (PDF/HTML/DOCX)
    â†“
Docling HybridChunker        â€” structure-aware chunking (respects headings, tables)
    â†“
MpetEmbedder                 â€” dense vectors (all-mpnet-base-v2, 768d)
    â†“
Store (config-driven)        â€” vectors + sparse index
    â†“
HybridRetriever              â€” dense + sparse + RRF fusion
    â†“
FlashRank                    â€” cross-encoder reranking
    â†“
Agentic Graph (LangGraph)
    â”œâ”€â”€ Router Node           â€” rag vs direct answer
    â”œâ”€â”€ RAG Node              â€” retrieval + generation
    â”œâ”€â”€ Critique Node         â€” grounding check + retry
    â””â”€â”€ Guard Nodes           â€” input/output safety
    â†“
Mistral-7B via Ollama        â€” local LLM, fully offline
    â†“
Streamlit UI                 â€” chat interface, linear/agentic toggle
```

---

## Multi-Backend Store (plug-n-play)

One of the strongest design decisions â€” abstract interfaces mean the entire retrieval
stack is swappable via a single `.env` change:

| Backend | Mode | Dense | Sparse | Fusion |
|---|---|---|---|---|
| **Supabase** | Production | pgvector | tsvector | SQL RRF (native) |
| **Qdrant** | Local dev | Vectors | BM42/FastEmbed | Qdrant RRF (native) |
| **Milvus + BM25S** | Fallback | Milvus-Lite | BM25S | Python RRF |

```bash
# Switch backends â€” zero code changes
STORE_BACKEND=supabase   # production
STORE_BACKEND=qdrant     # local dev
STORE_BACKEND=milvus     # offline fallback
```

---

## Project Structure

```
rag_project/
â”œâ”€â”€ cli/                          # Entrypoints (no __init__.py)
â”‚   â”œâ”€â”€ app.py                    # streamlit run cli/app.py
â”‚   â”œâ”€â”€ rag_query.py              # python cli/rag_query.py "question"
â”‚   â”œâ”€â”€ ingestion_pipeline.py     # python cli/ingestion_pipeline.py ./docs/
â”‚   â”œâ”€â”€ agentic_rag.py            # python cli/agentic_rag.py (REPL)
â”‚   â””â”€â”€ evaluate.py               # python cli/evaluate.py
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ interfaces.py             # Abstract base classes (VectorStoreBase etc.)
â”‚   â””â”€â”€ llm_client.py             # OpenAI-compatible LLM client
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ docling_chunker.py        # Docling HybridChunker wrapper
â”‚   â””â”€â”€ embedder.py               # MpetEmbedder (dense vectors)
â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ store_factory.py          # Registry â€” single source of truth for backends
â”‚   â”œâ”€â”€ supabase_store.py         # Supabase pgvector + tsvector
â”‚   â”œâ”€â”€ qdrant_store.py           # Qdrant dense + BM42 sparse
â”‚   â”œâ”€â”€ milvus_store.py           # Milvus-Lite dense
â”‚   â”œâ”€â”€ bm25_store.py             # BM25S sparse
â”‚   â”œâ”€â”€ sqlite_sparse_store.py    # SQLite FTS5 sparse (alternative to BM25S)
â”‚   â””â”€â”€ hybrid_retriever.py       # RRF fusion + FlashRank reranking
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ state.py                  # AgentState TypedDict
â”‚   â”œâ”€â”€ graph.py                  # LangGraph graph builder
â”‚   â”œâ”€â”€ router_node.py            # Route: rag vs direct
â”‚   â”œâ”€â”€ rag_node.py               # Retrieve + generate
â”‚   â””â”€â”€ critique_node.py          # Grounding check + retry
â”œâ”€â”€ guardrails/
â”‚   â”œâ”€â”€ guard_runner.py           # GuardRunner orchestrator
â”‚   â”œâ”€â”€ input_guards.py           # Injection, PII, topic, length
â”‚   â””â”€â”€ output_guards.py          # Toxicity, hallucination, PII redaction
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ ragas_evaluator.py        # RAGAS evaluation runner
â”‚   â”œâ”€â”€ failure_analyzer.py       # Per-question failure analysis
â”‚   â””â”€â”€ golden_set.json           # â† you create this
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py               # Pydantic-settings, all config
â”œâ”€â”€ pyproject.toml                # pip install -e . makes root importable
â””â”€â”€ .env                          # All secrets and config
```

---

## Key Design Patterns

### Abstract Interfaces (`core/interfaces.py`)
`VectorStoreBase`, `SparseStoreBase`, `EmbedderBase`, `RerankerBase` â€” every component
is swappable. Adding Pinecone = one new file implementing the interface.

### Store Factory (`retrieval/store_factory.py`)
Registry pattern â€” `build_pipeline()`, `build_retriever()`, `build_stores()`.
All entrypoints call one function. Adding a new backend = one new `_build_x()` function
plus one registry entry. Nothing else changes.

```python
BACKENDS = {
    "supabase": _build_supabase,
    "qdrant":   _build_qdrant,
    "milvus":   _build_milvus,
    # "pinecone": _build_pinecone,  â† future, one line
}
```

### Auto-detected Retrieval Path (`HybridRetriever`)
```
hasattr(store, "hybrid_search") â†’ Path A: SQL/native RRF (Supabase, Qdrant)
else                            â†’ Path B: Python RRF (Milvus + BM25S)
```
FlashRank reranks on top of whichever path produced the candidates.

### Config-driven Everything (`config/settings.py`)
Pydantic-settings â€” all backends, models, and parameters controlled via `.env`.
Each nested config declares `env_file` + `extra="ignore"` explicitly.

### CLI Entrypoints (`cli/`)
No `__init__.py` â€” scripts folder, not a package. `pyproject.toml` with
`pip install -e .` / `uv pip install -e .` makes the project root importable
from any entrypoint regardless of working directory.

---

## Hybrid Search â€” How It Works

Three stages, regardless of backend:

**1. Dense search** â€” cosine similarity on embedding vectors.
Good at paraphrasing, synonyms, semantic meaning.
Bad at exact keywords, version numbers, codes.

**2. Sparse search** â€” term frequency matching (BM25 / tsvector / BM42).
Good at exact matches, keywords, IDs.
Bad at paraphrasing and synonyms.

**3. RRF Fusion** â€” Reciprocal Rank Fusion combines both lists.
`score = Î£ 1/(k+rank)` â€” chunks appearing in both lists get a bonus.
`k=60` prevents top ranks from dominating.

**4. FlashRank** â€” cross-encoder sees query + chunk together.
Reranks fused candidates down to `top_k_rerank` (default 5).
Local, no API cost, fully offline.

---

## Agentic Pipeline (LangGraph)

```
input_guard â†’ router â†’ rag â†’ critique â†’ output_guard â†’ END
                        â†‘______â†“ (retry if not grounded, max 2x)
```

| Node | Reads | Writes | Decision |
|---|---|---|---|
| `input_guard` | query | blocked, warnings | Hard block or pass |
| `router` | query | route, reasoning | `rag` or `direct` |
| `rag` | query, route, retry_count | chunks, context, answer | Retrieves + generates |
| `critique` | query, context, answer | grounded, reasoning | Pass or retry |
| `output_guard` | answer | warnings, redactions | Final safety check |

**Retry logic** â€” on failed grounding, query is expanded with
`"(provide more detail and related context)"` before re-retrieval.
Max 2 retries, then disclaimer appended and pipeline exits.

---

## Streamlit UI

```
streamlit run cli/app.py
```

**Sidebar:**
- Ollama connectivity status (cached 30s)
- Pipeline mode toggle: `âš¡ Linear RAG` vs `ğŸ¤– Agentic RAG`
- Show sources / chunk content / agent trace toggles
- Clear chat button
- Ingestion command hint

**Agentic mode extras per response:**
- Route chosen + router reasoning
- Grounded âœ… / âŒ + critique reasoning
- Retry count
- Guard warnings / blocks

**Linear mode** â€” fast path, no guardrails, no critique. Same retrieval stack.

Both modes share the same cached `llm`, `retriever`, and `graph` instances â€”
switching modes doesn't reload anything.

---

## Good Points

**Unified store** â€” Supabase and Qdrant each replace two separate stores
(Milvus + BM25S). One connection, one client, native hybrid search.

**SQL RRF in Supabase** â€” one round trip instead of two searches + Python fusion.
Postgres handles the merge natively via CTE.

**Qdrant BM42** â€” neural sparse vectors via FastEmbed. Quality upgrade over
BM25 term frequency. Lazy-loaded and cached on the store instance â€” no repeated
model loading per query.

**FlashRank** â€” local cross-encoder, no API cost, no internet. Sits on top of
any retrieval path â€” the interface doesn't care what produced the candidates.

**`@st.cache_resource`** â€” pipeline built once per session. Both linear and
agentic share the same instances â€” no duplicate loading when toggling modes.

**Open/closed principle** â€” adding a new backend, embedder, or reranker never
requires modifying existing code â€” only adding new files and registry entries.

---

## Lessons Learned

**Pydantic nested configs don't inherit `env_file`**
Each nested `BaseSettings` must declare `env_file` + `extra="ignore"` explicitly.
Without it, nested configs only read process environment â€” not the `.env` file â€”
causing silent fallback to defaults (e.g. empty connection string â†’ local socket).

**Python has no method overloading**
Defining `search()` twice silently overwrites the first definition. Caused a
`"function plainto_tsquery(unknown, numeric[])"` Postgres error â€” the wrong
`search()` was being called with a float array. Solution: explicit naming
(`search_dense`, `search_sparse`).

**psycopg2 auto-deserializes JSONB**
`json.loads()` on an already-parsed dict throws `"must be str, bytes or bytearray, not dict"`.
Always check `isinstance(value, str)` before calling `json.loads()`.

**BM25S loads corpus into memory on every instantiation**
Fine for small corpora, a startup cost problem at scale. Supabase `tsvector` and
SQLite FTS5 solve this â€” the index lives on disk, queries hit it directly.

**LlamaIndex is glue, not capability**
`QueryFusionRetriever` fuses retrievers you explicitly provide â€” it doesn't conjure
sparse search on stores that don't support it natively. Qdrant and Supabase do hybrid
natively; Milvus-Lite and Chroma don't, regardless of framework wrapper.

**`encode_sparse()` is version-dependent**
`QdrantClient.encode_sparse()` only exists in newer client versions. Calling
FastEmbed's `SparseTextEmbedding` directly is more portable and version-stable.
Cache the encoder on the store instance to avoid reloading per query.

**Table noise is expected, not a bug**
Docling correctly extracts table rows as chunks â€” data rows without headers become
orphaned number sequences. They score low on FlashRank and rarely pollute answers.
Accept as a known limitation; revisit if RAGAS `context_precision` flags it.

**Streamlit ternary leaks DeltaGenerator**
`st.success(...) if ok else st.error(...)` evaluates both branches and the return
value renders as raw object internals in the UI. Always use `if/else` blocks for
Streamlit UI calls.

**Postgres pooler URL more reliable than direct connection**
Direct Supabase host (`db.[ref].supabase.co`) may not resolve on all networks.
Session pooler URL (`aws-0-[region].pooler.supabase.com:5432`) is more reliable.

---

## Retrieval Store Comparison

| | Supabase | Qdrant | Milvus-Lite | Elasticsearch |
|---|---|---|---|---|
| Dense | âœ… pgvector | âœ… Native | âœ… Native | âœ… Native |
| Sparse | âœ… tsvector | âœ… BM42 | âŒ Need BM25S | âœ… Native |
| Hybrid | âœ… SQL RRF | âœ… Native RRF | âŒ Python RRF | âœ… Native |
| Scalar filter | âœ… SQL WHERE | âœ… Payload | âš ï¸ Limited | âœ… Native |
| Local file | âŒ Cloud only | âœ… | âœ… | âŒ |
| Self-hosted | âœ… | âœ… | âœ… | âœ… |
| Ops complexity | Low | Low | Very low | High |

---

## What's Next

- **RAGAS evaluation** â€” build `evaluation/golden_set.json` (10-15 questions),
  run `cli/evaluate.py`, measure `faithfulness`, `context_recall`,
  `context_precision`, `answer_relevancy`
- **Better embedder** â€” `BAAI/bge-large-en-v1.5` (free, better MTEB) or
  `text-embedding-3-small` (OpenAI, paid, best quality) â€” both require re-ingestion
- **Pinecone backend** â€” `_build_pinecone()` in store factory, one `PineconeStore` file
- **Query expansion** â€” already stubbed in RAG node retry path, promote to first-class
- **Streaming** â€” `llm.stream()` + Streamlit `st.write_stream()` for token-by-token output
